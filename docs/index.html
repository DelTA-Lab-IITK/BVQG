
<html class="gr__filebox_ece_vt_edu"><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
	<style>
	body {
		background: white;
		color: #222222;
		padding: 0;
		margin: 0;
		font-family: "Helvetica Neue", "Helvetica", Helvetica, Arial, sans-serif;
		font-weight: normal;
		font-style: normal;
		line-height: 1.4;
   }
	a:link {
		text-decoration: none;
	}
	a:visited {
		text-decoration: none;
	}
	a:hover {
		text-decoration: underline;
	}
	a:active {
		text-decoration: underline;
	}
	p{
		font-size:1.2em;
	}
	img {
		width:100%;
		max-width: 300px;
	}
	.rcorner{
		border-radius: 25px;
		border: 2px solid #8AC007;
		padding: 20px;
	}
	div{
		max-width:1200px;
		position:relative;
		font-family: Arial;
	}
	.bold{font-weight: bold;}
	.italic{font-style: italic;}
	.sz25{font-size:2.5em;}
	.sz20{font-size:1.5em;}
	.sz15{font-size:1.5em;}
	.sz13{font-size:1.3em;}
	.sz12{font-size:1.2em;}
	.sz10{font-size:1.0em;}
	.sz08{font-size:0.8em;}
	.inline{display:inline-block;}
	.no_margin{
		margin: 0 0;
	}
	.padding20{
		padding: 0px;
	}
	/*.halign{*/
	/*	margin-left: auto;*/
	/*	margin-right: auto;*/
	/*}*/
	.block_2
	{
		width:49%;
		max-width: 512px;
		display: inline-block;
		position: relative;
	}
	.block_5_2
	{
		width:40%;
		max-width: 410px;
		display: inline-block;
		position: relative;
	}
	.block_3
	{
		width:33%;
		max_width: 341px;
		display: inline-block;
		position: relative;
	}
	.block_1
	{
		width:100%;
		display: inline-block;
		/*position: relative;*/
		/*margin: 5px 5px;*/
		text-align: justify;

	}
	.block_N
	{
		display: inline-block;
		position: relative;
	}
	.block_6
	{
		width:16%;
		display: inline-block;
		position: relative;

	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	.block_6_5
	{
		width:83%;
		display: inline-block;
		position: relative;
	}

	.std{
		border: 0.5px dashed black;
		border-collapse: collapse;
	}
	td {
		padding: 15px;
	}
		.link {
			color: #222222;
		}

	</style>
	<title>
		TWoPlayer
	</title>
	<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141788472-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141788472-1');
</script>

</head>
<body data-gr-c-s-loaded="true">
<div class="container">

<div style="margin-top: 20px">
	<div style="float: left; width: 10%; margin-left: 5%"><a href="http://deltalab.iitk.ac.in/"><img src="static/delta.png" alt="Delta Lab"/></a></div>
	<div style="float: left; width: 70%;">
	<div class="block_1">

		<div class="block_1">
			<div align="center">
				<p class="sz20 no_margin"><b></b>Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</p>
			</div>
			<p align="center">
			</p><p class="no_margin bold" align="center"> <a class="link" href="http://home.iitk.ac.in/~badri/index.html">Badri N. Patro</a><sup></sup>, <a class="link">Anupriy </a><sup></sup>, <a class="link" href="https://www.cse.iitk.ac.in/users/vinaypn/">Vinay P. Namboodiri</a></p>
<!--            <p  class="no_margin" align="center"><a href="http://deltalab.iitk.ac.in/">Delta Lab</a></p>-->
<!--				<p class="no_margin" align="center"><a href="http://iitk.ac.in/">Indian Institute of Technology Kanpur</a></p>-->
				<p class="sz12" align="center"><a class="link" href="http://arxiv.org/abs/1909.04800">[ArXiv]</a></a> <a>
			<p align="center">
			</p>
		</div>
	</div>
</div>
	<div style="float: left; width: 10%;"><a href="https://www.iitk.ac.in/"><img src="static/iitk.png"  alt="sherlock"/></a></div>
<!--<div class="halign">
		<div class="block_1">
			<div class="">

			</div>
	</div>
</div>-->

	<div class="block_1">
		<div class="">
			<br>
			<hr>
			<p class="sz15 bold">Abstract:</p>
			<div class="thumbnail" style="float:right; margin-left: 20px" >
					<img src="static/cvpr_motivation_new.png" alt="l0" style="max-width:400px">
					<div class="caption" style="max-width:400px">
						<p>This figure shows improvement of the Attention mask  and Explanation mask (Grad-CAM) using self supervised manner.</p>
					</div>
			</div>
				<p class="sz12">
					In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to
					provide supervision for attention. An observation we make is that visual explanations as obtained through class
					activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a
					means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be
					suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to
					distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions
					as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual
					explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps
					that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention
					network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can
					also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with
					other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD)
					and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the
					attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of
					supervision.</p>
		</div>
	</div>



	<p class="sz15 bold">
		<br>
		PAAN Model:
	</p>
	<div class="thumbnail" style="float:right; margin-left: 20px" >
		<img src="static/cvpr_complete.png" alt="l3">
		<hr>
		<div class="caption">
			<p>Illustration of model PAAN and its attention mask. Image feature and question feature are obtained using CNN and LSTM
			respectively. Attention mask is then obtained using these features and classification of the answer is done based on the
			attended feature. We have improved the attention mask with the visual explanation approaches based on Grad-CAM.</p>
		</div>
	</div>


	<p class="sz15 bold">
		Some example of visual question answering using our method:
	</p>


	<p class="sz15 bold">	
			<br>
			Attention Visualisation Results:	</p>
	<div class="thumbnail" style="float:right; margin-left: 20px">
		<img src="static/all_att_visual_1_5_9.jpg" alt="l5">
			<img src="static/all_att_visual_13_6_8.jpg" alt="l5">
		<hr>
		<div class="caption">
			<p>Examples with different approaches in each column for improving attention using explanation in a self supervised manner.
			The first column indicates the given target image and its question and answer. Starting from second column, it indicates
			the Attention map for Stack Attention Network, MSE based approach, Coral based approach,
			MMD based approach, Adversarial based approach respectively.</p>
		</div>
	</div>



<p class="sz15 bold">
	<br>
	Explanation Visualisation Results: </p>
<div class="thumbnail" style="float:right; margin-left: 20px">
	<img src="static/all_grad_visual_1_5_9.jpg" alt="l5">
	<img src="static/all_grad_visual_13_6_8.jpg" alt="l5">
	<hr>
	<div class="caption">
		<p>Examples with different approaches in each column for improving attention using explanation in a self supervised
			manner.
			The first column indicates the given target image and its question and answer. Starting from second column, it
			indicates
			the Grad-CAM map for Stack Attention Network, MSE based approach, Coral based
			approach,
			MMD based approach, Adversarial based approach respectively.</p>
	</div>
</div>


<p class="sz15 bold">
	<br>
	Warm-Start model Results: </p>
<div class="thumbnail" style="float:right; margin-left: 20px">
	<img src="static/att1.jpg" alt="l5">
	<img src="static/att2.jpg" alt="l5">
	<img src="static/att3.jpg" alt="l5">
	<hr>
	<div class="caption">
		<p>Visualisation of attention map after epoch-10, epoch-50, epoch-100, epoch-200.</p>
	</div>
</div>


<p class="sz15 bold">
	<br>
	Statistical Significance Analysis: </p>
<div class="thumbnail" style="float:right; margin-left: 20px">
	<img src="static/abalation.jpg" alt="l5">
	<hr>
	<div class="caption">
		<p>The mean rank of all the models on the basis of all scores are plotted on the x- axis. CD=3.3722, p=0.0003461. Here our
		PAAN model and others variants are described in section-4.1 of the main paper. The colored lines between the two models
		represents that these models are not significantly different from each other.</p>
	</div>
</div>




<p class="sz15 bold">
	<br>
	Visual Dialog Results for PAAN model: </p>
<div class="thumbnail" style="float:right; margin-left: 20px">
	<img src="static/grad_att_visdial.jpg" alt="l5">
	<hr>
	<div class="caption">
		<p>This figure shows visual explanation and attention map for Visual Dialog task. The first row contains grad cam results
		and second contains attention results for Adversarial approaches. We have shown all the dialog turns present in the
		dataset.</p>
	</div>
</div>



<p class="sz15 bold">
	<br>
	Attention map Variance : </p>
<div class="thumbnail" style="float:right; margin-left: 20px">
	<img src="static/image_final.jpg" alt="l5">
	<hr>
	<div class="caption">
		<p>Variance in attention map for the same question to the image and its composite image in VQA2.0 dataset.</p>
	</div>
</div>

<!-- 
### Dataset Details
##### VQA-V1
It contains human annotated question-answer pairs based on images on MS-COCO dataset's images.

| Field | Train | Val | Test |
|:---:|:---:|:---:|:---:|
Q-A pairs | 2,48,349 | 1,21,512 | 2,44,302 |

##### VQA-V2
It has almost twice the number of question-answer pairs as **VQA-V1** and also claims to remove some biases.

| Field | Train | Val | Test |
|:---:|:---:|:---:|:---:|
Q-A pairs | 4,43,757 | 2,14,354 | 4,47,793 |

##### VQA-HAT
Dataset containing human annotated attention maps. (Based on **VQA-V1**)


| Field | Train (out of 2,48,349) | Val (out of 1,21,512) |
|:---:|:---:|:---:|
Q-A pairs | 58,475 | 1,374 | -->

<style>
	table,
	th,
	td {
		border: 1px solid black;
		border-collapse: collapse;
	}
</style>


<h2>Dataset Details</h2>
<p class="sz15 bold">
	<br>
	VQA-V1: </p>
<p>It contains human annotated question-answer pairs based on images (MS-COCO dataset's images).</p>

<table style="width:40%">
	<tr>
		<th>Field</th>
		<th>Train</th>
		<th>Val</th>
		<th>Test</th>
	</tr>
	<tr>
		<td>Q-A pairs</td>
		<td>2,48,349</td>
		<td>1,21,512</td>
		<td>2,44,302</td>
	</tr>
</table>

<p class="sz15 bold">
	<br>
	VQA-V2: </p>
<p>It has almost twice the number of question-answer pairs as **VQA-V1** and also claims to remove some biases.</p>

	<table style="width:40%">
		<tr>
			<th>Field</th>
			<th>Train</th>
			<th>Val</th>
			<th>Test</th>
		</tr>
		<tr>
			<td>Q-A pairs</td>
			<td>4,43,757</td>
			<td>2,14,354</td>
			<td>4,47,793</td>
		</tr>
	</table>

<p class="sz15 bold">
	<br>
	VQA-HAT </p>
<p>Dataset containing human annotated attention maps. (Based on **VQA-V1**).</p>

<table style="width:40%">
	<tr>
		<th>Field</th>
		<th>Train (out of 2,48,349)</th>
		<th>Val (out of 1,21,512)</th>

	</tr>
	<tr>
		<td>Q-A pairs</td>
		<td>58,475</td>
		<td>1,374</td>
	</tr>
</table>
	<!-- This is for empty space -->
<p>                                              </p>
<p>-----------------------------------------------</p>
</div>
</div>
<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141358857-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141358857-1');
</script>


<script type="text/javascript">

</script>

</body></html>
